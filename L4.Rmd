---
title: "LR4"
author: "Kovel"
date: '30 РѕРєС‚СЏР±СЂСЏ 2020 Рі '
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Download the files
```{r}
f_train <- read.csv2('indeks_train.csv', header = TRUE, encoding = 'UNICOD')
f_test <- read.csv2('indeks_test.csv', header = TRUE, encoding = 'UNICOD')
```
##Висновок: окремо завантажені навчальна і тестова вибірки.

#Decision Tree Regression
#Fitting simple tree
```{r}
# install.packages('rpart')
library(rpart)
dt <- rpart(Y ~ X1, f_train, control = rpart.control(minsplit = 20))
plot(dt)
text(dt, pos = 1, cex = .75, col = 1, font = 1)
```
##Висновок:побудовано дерево рішень, екзогенна змінна – X1.
#Predicting
```{r}
p_dt <- predict(dt, f_test)

train_mse_dt <- sum((f_train$Y-predict(dt, f_train))^2) /length(f_train$Y)
test_mse_dt <- sum((f_test$Y-p_dt)^2)/length(p_dt)

train_mse_dt
test_mse_dt
```
##Висновок:значення середньоквадратичної похибки трохи покращилися на навчальній вибірці – 0.01301742 (було 0.01334668), погіршилися на тестовій вибірці – 0.02230507(було 0.008486525). Модель перенавчено.
#Visualising
```{r}
library(ggplot2)
x_grid <- seq(min(f_train$X1), max(f_train$X1), 0.01)
ggplot() +
  geom_point(aes(f_train$X1, f_train$Y),colour = 'red') +
  geom_point(aes(f_test$X1, f_test$Y),colour = 'dark green') +
  geom_line(aes(x_grid, predict(dt, data.frame(X1 = x_grid))),colour = 'blue') +
  ggtitle('Y vs X1') +
  xlab('X1') +
  ylab('Y')
```
##Висновок:Висновок: на графіку червоним позначені точки навчальної вибірки, зеленим – точки тестової вибірки, синім – модельні значення.
#Fitting full tree
```{r}
# install.packages('rpart')
library(rpart)
dt <- rpart(Y ~ t + X1 + X3 + X4, f_train, control = rpart.control(minsplit = 20))
plot(dt)
text(dt, pos = 1, cex = .75, col = 1, font = 1)
```
##Висновок:побудовано дерево рішень, екзогенні змінні – t,X1,X3.X4
#Predicting
```{r}
p_dt <- predict(dt, f_test)

train_mse_dt <- sum((f_train$Y-predict(dt, f_train))^2)/length(f_train$Y)
test_mse_dt <- sum((f_test$Y-p_dt)^2)/length(p_dt)

train_mse_dt
test_mse_dt
```
##Висновок:значення середньоквадратичної похибки трохи покращилися на навчальній вибірці – 0.009771367 (було 0.01301742), також є покращення і на тестовій вибірці – 0.0137824(було 0.02230507). Та модель все одно перенавчено.
#Visualising
#Random forest
#Fitting
```{r}
# install.packages('randomForest')
library(randomForest)
set.seed(1234)
rf = randomForest(x = f_train['X1'],
                         y = f_train$Y,
                         ntree = 5)
```
##Висновок: побудовано віпадковий ліс із 5 дерев, екзогенна змінна – X1.
#Predicting
```{r}
p_rf <- predict(rf, f_test)

train_mse_rf <- sum((f_train$Y-predict(rf, f_train))^2)/length(f_train$Y)
test_mse_rf <- sum((f_test$Y-p_rf)^2)/length(p_rf)

train_mse_rf
test_mse_rf
```
##Висновок: значення середньоквадратичної похибки покращилися на навчальній вибірці – 0.005588631 (було 0.009771367), трошки погіршилися на тестовій вибірці – 0.01614003 (було 0.0137824). Модель перенавчено.
#Visualising
```{r}
ggplot() +
  geom_point(aes(f_train$X1, f_train$Y),colour = 'red') +
  geom_point(aes(f_test$X1, f_test$Y),colour = 'dark green') +
  geom_line(aes(x_grid, predict(rf, data.frame(X1 = x_grid))),colour = 'blue') +
  ggtitle('Y vs X1') +
  xlab('X1') +
  ylab('Y')
```
##Висновок: на графіку червоним позначені точки навчальної вибірки, зеленим – точки тестової вибірки, синім – модельні значення.
#Saving results
```{r}
fit <- read.csv2('indeks_fit.csv', header = TRUE, encoding = 'UNICOD')
fit$p_dt <- p_dt
fit$p_rf <- p_rf
head(fit)
write.csv2(fit[-1], file = "indeks_fit.csv")
```
##Висновок:збережено результати