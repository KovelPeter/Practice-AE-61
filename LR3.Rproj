---
title: "LR3"
author: "Kovel"
date: '29 октября 2020 г '
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Download the data
```{r}
#Download the files
f_train <- read.csv2('indeks_train.csv', header = TRUE, encoding = 'UNICOD')
f_train <- f_train[,-1]
f_test <- read.csv2('indeks_test.csv', header = TRUE, encoding = 'UNICOD')
f_test <- f_test[,-1]
```
#Висновок: окремо задані навчальна і тестова вибірки, видалені перші стовпчики з індексами об’єктів до кожної з підвибірок.

#Simple Linear Regression (one factor – X1)

## Fitting Simple Linear Regression to the Training set
```{r}
model_sr <- lm(Y ~ X1, f_train)
summary(model_sr)
```
##Висновок: обрана змінна X1 значуща, коефіцієнт детермінації 0,62 - не великий,тобто змінна Х1 не дуже гарно описує змінну У.
```{r}
p_sr <- predict(model_sr, f_test)

r2_sr <- 1-sum((f_train$Y - predict(model_sr, f_train))^2)/sum((f_train$Y - mean(f_train$Y))^2)
R2_sr <- cor(f_train$Y, fitted(model_sr))^2 #simplier ex.

train_mse_sr <- sum((f_train$Y-predict(model_sr, f_train))^2)/length(f_train$Y)
test_mse_sr <- sum((f_test$Y-p_sr)^2)/length(p_sr)
r2_sr
R2_sr
train_mse_sr
test_mse_sr
```


##Висновок: вручну розраховані коефіцієнти детермінації,вони рівні. Значення середньоквадратичної похибки на навчальній вибірці – 0.01785301, на тестовій вибірці – 0.01066023, тобто перенавчання немає.

## Visualising
```{r}
library(ggplot2)
ggplot() +
  geom_point(aes(f_train$X1, f_train$Y),colour = 'red') +
  geom_point(aes(f_test$X1, f_test$Y),colour = 'dark green') +
  geom_line(aes(f_test$X1, p_sr),colour = 'blue') +
  ggtitle('Y vs X1') +
  xlab('X1') +
  ylab('Y')
```
##Висновок: на графіку червоним позначені точки навчальної вибірки, зеленим – точки тестової вибірки, синім – модельні значення.
# Multiple Linear Regression (many factors)

## All factors
```{r}
model_mr <- lm(Y ~ ., f_train) 
summary(model_mr)  
```
##Висновок:змінна X2 найменш значуща, а найбільш, як і визначились раніше,Х1; коефіцієнт детермінації дорівнює 0,73, він збільшився, що говорить про те,що включення усіх змінних до моделі призвело до того,що модель стала кращою, змінна У описується заданними змінними.

## Optimized model
```{r}
#as p-value, Pr(>|t|) of variable "type" is higher than significance level (5%), let's exclude this variable from the model
mr_opt <- lm(Y ~ t + X1 + X3 + X4, f_train) 
summary(mr_opt)  
```
##Висновок: усі змінні значущі, коефіцієнт детермінації трохи зменшився – 0,72.
## Prediction
```{r}
p_mr <- predict(mr_opt, f_test)

train_mse_opt <- sum((f_train$Y-predict(mr_opt, f_train))^2)/length(f_train$Y)
test_mse_opt <- sum((f_test$Y-p_mr)^2)/length(p_mr)

train_mse_opt
test_mse_opt
```
##Висновок: значення середньоквадратичної помилки покращилися – на навчальній вибірці – 0.01334668, на тестовій вибірці – 0.008486525, тобто перенавчання немає.

## Visualising
```{r}
ggplot() +
  geom_point(aes(f_train$X1, f_train$Y),colour = 'red') +
  geom_point(aes(f_test$X1, f_test$Y),colour = 'dark green') +
  geom_line(aes(f_test$X1, p_mr),colour = 'blue') +
  ggtitle('Y vs X1') +
  xlab('X1') +
  ylab('Y')
```
##Висновок: на графіку червоним позначені точки навчальної вибірки, зеленим – точки тестової вибірки, синім – модельні значення.
# Polynomial Linear Regression (one factor - X1)

## Features extending 
```{r}
f_train_poly <- f_train[,c('Y', 'X1')]
f_test_poly <- f_test[,c('Y', 'X1')]
f_train_poly$X12 <- f_train_poly$X1^2
f_train_poly$X13 <- f_train_poly$X1^3
f_test_poly$X12 <- f_test_poly$X1^2
f_test_poly$X13 <- f_test_poly$X1^3
```
##Висновок:
## 3 powers
```{r}
pr <- lm(Y ~ X12 + X13, f_train_poly) 
summary(pr)  
```
##Висновок:
## Predicting
```{r}
p_pr <- predict(pr, f_test_poly)

train_mse_poly <- sum((f_train_poly$Y-predict(model_pr, f_train_poly))^2)/length(f_train_poly$Y)
test_mse_poly <- sum((f_test_poly$Y-p_pr)^2)/length(p_pr)

train_mse_poly
test_mse_poly
```
##Висновок:
## Visualising
```{r}
ggplot() +
  geom_point(aes(f_train_poly$X1, f_train_poly$Y),colour = 'red') +
  geom_point(aes(f_test_poly$X1, f_test_poly$Y),colour = 'dark green') +
  geom_line(aes(f_test_poly$X1, p_pr),colour = 'blue') +
  ggtitle('Y vs X1') +
  xlab('X1') +
  ylab('Y')
```
##Висновок:
# Saving results
```{r}
fit <- data.frame(p_sr, p_mr, p_pr)
write.csv2(fit, file = "indeks_fit.csv")
```
##Висновок: